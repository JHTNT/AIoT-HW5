import math
from typing import Dict, Iterable, List

import streamlit as st
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline
from transformers import logging

logging.set_verbosity_error()

MODEL_NAME = "openai-community/roberta-base-openai-detector"
AI_LABEL_HINTS = {"AI", "FAKE", "MACHINE", "GENERATED", "LABEL_1", "BOT"}
HUMAN_LABEL_HINTS = {"HUMAN", "REAL", "LABEL_0"}


@st.cache_resource(show_spinner=False)
def load_detector():
    """Load the HF pipeline once and cache it for reuse."""
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)
    return pipeline(
        task="text-classification",
        model=model,
        tokenizer=tokenizer,
        device=-1,  # force CPU for broad compatibility
    )


def score_to_percent(ai_score: float, human_score: float) -> Dict[str, float]:
    total = ai_score + human_score
    if total <= 0:
        return {"ai": 50.0, "human": 50.0}
    return {"ai": 100 * ai_score / total, "human": 100 * human_score / total}


def aggregate(scores: Iterable[Dict[str, float]]):
    ai_score = 0.0
    human_score = 0.0
    for row in scores:
        label = row.get("label", "").upper()
        score = float(row.get("score", 0.0))
        if any(hint in label for hint in AI_LABEL_HINTS):
            ai_score += score
        elif any(hint in label for hint in HUMAN_LABEL_HINTS):
            human_score += score
    # fallback: assume the top label represents AI if hints failed
    if ai_score == 0 and human_score == 0 and scores:
        best = max(scores, key=lambda r: r.get("score", 0))
        ai_score = float(best.get("score", 0.5))
        human_score = 1.0 - ai_score
    return ai_score, human_score


def format_scores(scores: List[Dict[str, float]]):
    if not scores:
        return "No scores returned."
    return "\n".join(
        f"{row.get('label', '?')}: {row.get('score', 0):.4f}" for row in scores
    )


st.set_page_config(
    page_title="AI vs Human Detector",
    page_icon="ğŸ¤–",
    layout="wide",
)

st.title("AI vs Human æ–‡æœ¬åˆ¤æ–·")
st.caption("ä½¿ç”¨ Hugging Face roberta-base-openai-detector é€²è¡Œå¿«é€Ÿæ¨è«–ã€‚")

pipe = load_detector()

example_text = """
Large language models can draft text quickly, but human writers add nuance and context that models may miss.
""".strip()

input_text = st.text_area(
    "è¼¸å…¥å¾…åˆ¤æ–·çš„æ–‡æœ¬ï¼š",
    value=example_text,
    height=220,
    placeholder="è²¼ä¸Šæ–‡ç« æˆ–è¼¸å…¥å¥å­...",
)

col_run, col_clear = st.columns([2, 1])

with col_run:
    run_detection = st.button("åˆ¤æ–·", type="primary")
with col_clear:
    clear = st.button("æ¸…ç©º")

if clear:
    st.experimental_rerun()

if run_detection and input_text.strip():
    with st.spinner("æ¨¡å‹æ¨è«–ä¸­..."):
        result = pipe(input_text, top_k=None)

    if isinstance(result, list):
        # If batching is ever used, the pipeline returns a list per input.
        if result and isinstance(result[0], list):
            scores = result[0]
        else:
            scores = result
    else:
        scores = [result]
    ai_score, human_score = aggregate(scores)
    percents = score_to_percent(ai_score, human_score)

    st.subheader("çµæœ")
    cols = st.columns(2)
    with cols[0]:
        st.metric("AI å¯èƒ½æ€§", f"{percents['ai']:.1f}%")
        st.progress(min(1.0, percents["ai"] / 100.0))
    with cols[1]:
        st.metric("Human å¯èƒ½æ€§", f"{percents['human']:.1f}%")
        st.progress(min(1.0, percents["human"] / 100.0))

    verdict = "AI ç”Ÿæˆ" if percents["ai"] >= percents["human"] else "Human æ’°å¯«"
    st.info(f"æ¨æ¸¬çµæœï¼š{verdict}")

    with st.expander("æŸ¥çœ‹æ¨¡å‹å®Œæ•´åˆ†æ•¸"):
        st.text(format_scores(scores))

    with st.expander("ç°¡æ˜“çµ±è¨ˆ"):
        words = input_text.split()
        st.write(
            {
                "å­—å…ƒæ•¸": len(input_text),
                "å–®è©æ•¸": len(words),
                "å¹³å‡å–®è©é•·åº¦": round(
                    sum(len(w) for w in words) / max(len(words), 1), 2
                ),
            }
        )
else:
    st.caption("é»æ“Šä¸Šæ–¹ \"åˆ¤æ–·\" æŒ‰éˆ•ä»¥å–å¾—çµæœã€‚")

st.sidebar.header("ä½¿ç”¨èªªæ˜")
st.sidebar.markdown(
    "- è²¼ä¸Šå¾…åˆ†ææ–‡æœ¬ï¼Œé»æ“Š **åˆ¤æ–·**ã€‚\n"
    "- é¡¯ç¤º AI / Human æ¦‚ç‡èˆ‡åˆ¤æ–·ã€‚\n"
    "- æ¨¡å‹åƒ…æä¾›åƒè€ƒï¼Œé•·æ–‡æœ¬æ•ˆæœè¼ƒä½³ã€‚"
)

st.sidebar.divider()
st.sidebar.markdown(
    "æ¨¡å‹ä¾†æº: [roberta-base-openai-detector](https://huggingface.co/openai-community/roberta-base-openai-detector)"
)
